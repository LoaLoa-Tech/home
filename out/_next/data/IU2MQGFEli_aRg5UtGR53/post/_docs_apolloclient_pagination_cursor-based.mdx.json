{"pageProps":{"markdownPost":{"name":null,"url":null,"content":"---\ntitle: 'Cursor-based pagination'\nsidebar_title: 'Cursor-based'\n---\n\n> We recommend reading [Core pagination API](./core-api) before learning about considerations specific to cursor-based pagination.\n\n## Using list element IDs as cursors\n\nSince numeric offsets within paginated lists can be unreliable, a common improvement is to identify the beginning of a page using some unique identifier that belongs to each element of the list.\n\nIf the list represents a set of elements without duplicates, this identifier could simply be the unique ID of each object, allowing additional pages to be requested using the ID of the last object in the list, together with some `limit` argument. With this approach, the requested `cursor` ID should not appear in the new page, since it identifies the item just before the beginning of the page.\n\nSince the elements of the list could be normalized `Reference` objects, you will probably want to use the `options.readField` helper function to read the `id` field in your `merge` and/or `read` functions:\n\n```js\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Query: {\n      fields: {\n        feed: {\n          keyArgs: [\"type\"],\n\n          merge(existing, incoming, {\n            args: { cursor },\n            readField,\n          }) {\n            const merged = existing ? existing.slice(0) : [];\n            let offset = offsetFromCursor(merged, cursor, readField);\n            // If we couldn't find the cursor, default to appending to\n            // the end of the list, so we don't lose any data.\n            if (offset < 0) offset = merged.length;\n            // Now that we have a reliable offset, the rest of this logic\n            // is the same as in offsetLimitPagination.\n            for (let i = 0; i < incoming.length; ++i) {\n              merged[offset + i] = incoming[i];\n            }\n            return merged;\n          },\n\n          // If you always want to return the whole list, you can omit\n          // this read function.\n          read(existing, {\n            args: { cursor, limit = existing.length },\n            readField,\n          }) {\n            if (existing) {\n              let offset = offsetFromCursor(existing, cursor, readField);\n              // If we couldn't find the cursor, default to reading the\n              // entire list.\n              if (offset < 0) offset = 0;\n              return existing.slice(offset, offset + limit);\n            }\n          },\n        },\n      },\n    },\n  },\n});\n\nfunction offsetFromCursor(items, cursor, readField) {\n  // Search from the back of the list because the cursor we're\n  // looking for is typically the ID of the last item.\n  for (let i = items.length - 1; i >= 0; --i) {\n    const item = items[i];\n    // Using readField works for both non-normalized objects\n    // (returning item.id) and normalized references (returning\n    // the id field from the referenced entity object), so it's\n    // a good idea to use readField when you're not sure what\n    // kind of elements you're dealing with.\n    if (readField(\"id\", item) === cursor) {\n      // Add one because the cursor identifies the item just\n      // before the first item in the page we care about.\n      return i + 1;\n    }\n  }\n  // Report that the cursor could not be found.\n  return -1;\n}\n```\n\nSince items can be removed from, added to, or moved around within the list without altering their `id` fields, this pagination strategy tends to be more resilient to list mutations than the `offset`-based strategy we saw above.\n\nHowever, this strategy works best when your `merge` function always appends new pages to the existing data, since it doesn't take any precautions to avoid overwriting elements if the `cursor` falls somewhere in the middle of the existing data.\n\n## Using a map to store unique items\n\nIf your paginated field logically represents a _set_ of unique items, you can store it internally using a more convenient data structure than an array.\n\nIn fact, your `merge` function can return internal data in any format you like, as long as your `read` function cooperates by turning that internal representation back into a list:\n\n```js\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Query: {\n      fields: {\n        feed: {\n          keyArgs: [\"type\"],\n\n          // While args.cursor may still be important for requesting\n          // a given page, it no longer has any role to play in the\n          // merge function.\n          merge(existing, incoming, { readField }) {\n            const merged = { ...existing };\n            incoming.forEach(item => {\n              merged[readField(\"id\", item)] = item;\n            });\n            return merged;\n          },\n\n          // Return all items stored so far, to avoid ambiguities\n          // about the order of the items.\n          read(existing) {\n            return existing && Object.values(existing);\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nWith this internal representation, you no longer have to worry about incoming items overwriting unrelated existing items, because an assignment to the map can only replace an item with the same `id` field.\n\nHowever, this approach leaves an important question unanswered: what `cursor` should we use when requesting the _next_ page? Thanks to the predictable ordering of JavaScript object keys by insertion order, you should be able to use the `id` field of the last element returned by the `read` function as the `cursor` for the next request&mdash;though you're not alone if relying on this behavior makes you nervous. In the next section we'll see a slightly different approach that makes the next `cursor` more explicit.\n\n## Keeping cursors separate from items\n\nPagination cursors are often derived from ID fields of list items, but not always. In cases where the list could have duplicates, or is sorted or filtered according to some criteria, the cursor may need to encode not just a position within the list but also the sorting/filtering logic that produced the list. In such situations, since the cursor does not logically belong to the elements of the list, the cursor may be returned separately from the list:\n\n```jsx\nconst MORE_COMMENTS_QUERY = gql`\n  query MoreComments($cursor: String, $limit: Int!) {\n    moreComments(cursor: $cursor, limit: $limit) {\n      cursor\n      comments {\n        id\n        author\n        text\n      }\n    }\n  }\n`;\n\nfunction CommentsWithData() {\n  const {\n    data,\n    loading,\n    fetchMore,\n  } = useQuery(MORE_COMMENTS_QUERY, {\n    variables: { limit: 10 },\n  });\n\n  if (loading) return <Loading/>;\n\n  return (\n    <Comments\n      entries={data.moreComments.comments || []}\n      onLoadMore={() => fetchMore({\n        variables: {\n          cursor: data.moreComments.cursor,\n        },\n      })}\n    />\n  );\n}\n```\n\nTo demonstrate the flexibility of the field policy system, here's an implementation of the `Query.moreComments` field that uses a map internally, but returns an array of unique `comments`:\n\n```js\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Query: {\n      fields: {\n        moreComments: {\n          merge(existing, incoming, { readField }) {\n            const comments = existing ? { ...existing.comments } : {};\n            incoming.comments.forEach(comment => {\n              comments[readField(\"id\", comment)] = comment;\n            });\n            return {\n              cursor: incoming.cursor,\n              comments,\n            };\n          },\n\n          read(existing) {\n            if (existing) {\n              return {\n                cursor: existing.cursor,\n                comments: Object.values(existing.comments),\n              };\n            }\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nNow there's less ambiguity about where the next `cursor` comes from, because it is explicitly stored and returned as part of the query.\n\n## Relay-style cursor pagination\n\nThe `InMemoryCache` field policy API allows for any conceivable style of pagination, even though some of the simpler approaches have known drawbacks.\n\nIf you were designing a GraphQL client without the flexibility that `read` and `merge` functions provide, you would most likely attempt to standardize around a one-size-fits-all style of pagination that you felt was sophisticated enough to support most use cases. That's the path Relay, another popular GraphQL client, has chosen with their [Cursor Connections Specification](https://facebook.github.io/relay/graphql/connections.htm). As a consequence, a number of public GraphQL APIs have adopted the Relay connection specification to be maximally compatible with Relay clients.\n\nUsing Relay-style connections is similar to cursor-based pagination, but differs in the format of the query response, which affects the way cursors are managed. In addition to `connection.edges`, which is a list of `{ cursor, node }` objects, where each `edge.node` is a list item, Relay provides a `connection.pageInfo` object which gives the cursors of the first and last items in `connection.edges` as `connection.pageInfo.startCursor` and `connection.pageInfo.endCursor`, respectively. The `pageInfo` object also contains the boolean properties `hasPreviousPage` and `hasNextPage`, which can be used to determine if there are more results available (both forwards and backwards):\n\n```jsx\nconst COMMENTS_QUERY = gql`\n  query Comments($cursor: String) {\n    comments(first: 10, after: $cursor) {\n      edges {\n        node {\n          author\n          text\n        }\n      }\n      pageInfo {\n        endCursor\n        hasNextPage\n      }\n    }\n  }\n`;\n\nfunction CommentsWithData() {\n  const {\n    data,\n    loading,\n    fetchMore,\n  } = useQuery(COMMENTS_QUERY);\n\n  if (loading) return <Loading/>;\n\n  const nodes = data.comments.edges.map(edge => edge.node);\n\n  return (\n    <Comments\n      entries={nodes}\n      onLoadMore={() => fetchMore({\n        variables: {\n          cursor: data.comments.pageInfo.endCursor,\n        },\n      })}\n    />\n  );\n}\n```\n\nFortunately, Relay-style pagination can be implemented in Apollo Client using `merge` and `read` functions, which means all the thorny details of connections and `edges` and `pageInfo` can be abstracted away, into a single, reusable helper function:\n\n```js\nimport { relayStylePagination } from \"@apollo/client/utilities\";\n\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Query: {\n      fields: {\n        comments: relayStylePagination(),\n      },\n    },\n  },\n});\n```\n\nWhenever you need to consume a Relay pagination API using Apollo Client, `relayStylePagination` is a great tool to try first, even if you end up copy/pasting its code and making changes to suit your specific needs.\n\nNote that the `relayStylePagination` function generates a field policy with a `read` function that simply returns all available data, ignoring `args`, which makes `relayStylePagination` easier to use with `fetchMore`. This is a [non-paginated `read` function](./core-api/#non-paginated-read-functions). There's nothing stopping you from adapting this `read` function to use `args` to return individual pages, as long as you remember to update the variables of your original query after calling `fetchMore`.\n"}},"__N_SSG":true}