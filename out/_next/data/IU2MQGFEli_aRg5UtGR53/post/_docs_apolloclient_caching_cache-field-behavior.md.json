{"pageProps":{"markdownPost":{"name":null,"url":null,"content":"---\ntitle: Customizing the behavior of cached fields\nsidebar_title: Customizing field behavior\n---\n\nYou can customize how a particular field in your Apollo Client cache is read and written. To do so, you define a **field policy** for the field. A field policy can include:\n\n* A [`read` function](#the-read-function) that specifies what happens when the field's cached value is read\n* A [`merge` function](#the-merge-function) that specifies what happens when field's cached value is written\n* An array of [key arguments](#specifying-key-arguments) that help the cache avoid storing unnecessary duplicate data.\n\nYou provide field policies to the constructor of `InMemoryCache`. Each field policy is defined inside whatever [`TypePolicy` object](./cache-configuration/#typepolicy-fields)  corresponds to the type that contains the field. The following example defines a field policy for the `name` field of a `Person` type: \n\n```ts{5-10}\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Person: {\n      fields: {\n        name: {\n          read(name) {\n            // Return the cached name, transformed to upper case\n            return name.toUpperCase();\n          }\n        }\n      },\n    },\n  },\n});\n```\n\nThe field policy above defines a [`read` function](#the-read-function) that specifies what the cache returns whenever `Person.name` is queried.\n\n## The `read` function\n\nIf you define a `read` function for a field, the cache calls that function whenever your client queries for the field. In the query response, the field is populated with the `read` function's return value, _instead of the field's cached value_.\n\nThe first parameter of a `read` function provides the field's currently cached value, if one exists. You can use this to help determine the function's return value.\n\nThe second parameter is an object that provides access to several properties and helper functions, which are explained in the [`FieldPolicy` API reference](#fieldpolicy-api-reference).\n\nThe following `read` function assigns a default value of `UNKNOWN NAME` to the `name` field of a `Person` type, if the actual value is not available in the cache. In all other cases, the cached value is returned.\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Person: {\n      fields: {\n        name: {\n          read(name = \"UNKNOWN NAME\") {\n            return name;\n          }\n        },\n      },\n    },\n  },\n});\n```\n\nIf a field accepts arguments, the second parameter includes the values of those arguments. The following `read` function checks to see if the `maxLength` argument is provided when the `name` field is queried. If it is, the function returns only the first `maxLength` characters of the person's name. Otherwise, the person's full name is returned.\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Person: {\n      fields: {\n        // If a field's TypePolicy would only include a read function,\n        // you can optionally define the function like so, instead of\n        // nesting it inside an object as shown in the example above.\n        name(name: string, { args }) {\n          if (args && typeof args.maxLength === \"number\") {\n            return name.substring(0, args.maxLength);\n          }\n          return name;\n        },\n      },\n    },\n  },\n});\n```\n\nYou can define a `read` function for a field that isn't even defined in your schema. For example, the following `read` function enables you to query a `userId` field that is always populated with locally stored data:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Person: {\n      fields: {\n        userId() {\n          return localStorage.getItem(\"loggedInUserId\");\n        },\n      },\n    },\n  },\n});\n```\n\n> Note that to query for a field that is only defined locally, your query should [include the `@client` directive](../local-state/managing-state-with-field-policies#querying) on that field so that Apollo Client doesn't include it in requests to your GraphQL server.\n\nOther use cases for a `read` function include:\n\n* Transforming cached data to suit your client's needs, such as rounding floating-point values to the nearest integer\n* Deriving local-only fields from one or more schema fields on the same object (such as deriving an `age` field from a `birthDate` field)\n* Deriving local-only fields from one or more schema fields across _multiple_ objects\n\nFor a full list of the options provided to the `read` function, see the [API reference](#fieldpolicy-api-reference). You will almost never need to use all of these options, but each one has an important role when reading fields from the cache.\n\n## The `merge` function\n\nIf you define a `merge` function for a field, the cache calls that function whenever the field is about to be written with an incoming value (such as from your GraphQL server). When the write occurs, the field's new value is set to the `merge` function's return value, _instead of the original incoming value_.\n\n### Merging arrays\n\nA common use case for a `merge` function is to define how to write to a field that holds an array. By default, the field's existing array is _completely replaced_ by the incoming array. Often, it's preferable to _concatenate_ the two arrays instead, like so:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Agenda: {\n      fields: {\n        tasks: {\n          merge(existing = [], incoming: any[]) {\n            return [...existing, ...incoming];\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nNote that `existing` is undefined the very first time this function is called for a given instance of the field, because the cache does not yet contain any data for the field. Providing the `existing = []` default parameter is a convenient way to handle this case.\n\n> Your `merge` function **cannot** push the `incoming` array directly onto the `existing` array. It must instead return a new array to prevent potential errors. In development mode, Apollo Client prevents unintended modification of the `existing` data with `Object.freeze`.\n\n### Merging non-normalized objects\n\nAnother common use case for custom field `merge` functions is to combine nested objects that do not have IDs, but are known (by you, the application developer) to represent the same logical object, assuming the parent object is the same.\n\nSuppose that a `Book` type has an `author` field, which is an object containing information like the author's `name`, `language`, and `dateOfBirth`. The `Book` object has `__typename: \"Book\"` and a unique `isbn` field, so the cache can tell when two `Book` result objects represent the same logical entity. However, for whatever reason, the query that retrieved this `Book` did not ask for enough information about the `book.author` object. Perhaps no `keyFields` were specified for the `Author` type, and there is no default `id` field.\n\nThis lack of identifying information poses a problem for the cache, because it cannot determine automatically whether two `Author` result objects are the same. If multiple queries ask for different information about the author of this `Book`, the order of the queries matters, because the `favoriteBook.author` object from the second query cannot be safely merged with the `favoriteBook.author` object from the first query, and vice-versa:\n\n```graphql\nquery BookWithAuthorName {\n  favoriteBook {\n    isbn\n    title\n    author {\n      name\n    }\n  }\n}\n\nquery BookWithAuthorLanguage {\n  favoriteBook {\n    isbn\n    title\n    author {\n      language\n    }\n  }\n}\n```\n\nIn such situations, the cache defaults to _replacing_ the existing `favoriteBook.author` data with the incoming data, without merging the `name` and `language` fields together, because the risk of merging inconsistent `name` and `language` fields from different authors is unacceptable.\n\n> Note: Apollo Client 2.x would sometimes merge unidentified objects. While this behavior might accidentally have aligned with the intentions of the developer, it led to subtle inconsistencies within the cache. Apollo Client 3.0 refuses to perform unsafe merges, and instead warns about potential loss of unidentified data.\n\nYou could fix this problem by modifying your queries to request an `id` field for the `favoriteBook.author` objects, or by specifying custom `keyFields` in the `Author` type policy, such as `[\"name\", \"dateOfBirth\"]`. Providing the cache with this information allows it to know when two `Author` objects represent the same logical entity, so it can safely merge their fields. This solution is recommended, when feasible.\n\nHowever, you may encounter situations where your data graph does not provide any uniquely identifying fields for `Author` objects. In these rare scenarios, it might be safe to assume that a given `Book` has one and only one primary `Author`, and the author never changes. In other words, the identity of the author is implied by the identity of the book. This common-sense knowledge is something you have at your disposal, as a human, but it must be communicated to the cache, which is neither human nor capable of telepathy.\n\nIn such situations, you can define a custom `merge` function for the `author` field within the type policy for `Book`:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        author: {\n          merge(existing, incoming) {\n            // Better, but not quite correct.\n            return { ...existing, ...incoming };\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nAlternatively, if you prefer to keep the default behavior of completely replacing the `existing` data with the `incoming` data, while also silencing the warnings, the following `merge` function will explicitly permit replacement:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        author: {\n          merge(existing, incoming) {\n            // Equivalent to what happens if there is no custom merge function.\n            return incoming;\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nSince writing this kind of `merge` function can become repetitive, the following shorthand will provide the same behavior:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        author: {\n          // Short for always preferring incoming over existing data.\n          merge: false,\n        },\n      },\n    },\n  },\n});\n```\n\nWhen you use `{ ...existing, ...incoming }`, `Author` objects with differing fields (`name`, `dateOfBirth`) can be combined without losing fields, which is definitely an improvement over blind replacement.\n\nBut what if the `Author` type defines its own custom `merge` functions for fields of the `incoming` object? Since we're using [object spread syntax](https://2ality.com/2016/10/rest-spread-properties.html), such fields will immediately overwrite fields in `existing`, without triggering any nested `merge` functions. The `{ ...existing, ...incoming }` syntax may be an improvement, but it is not fully correct.\n\nFortunately, you can find a helper function called `options.mergeObjects` in the options passed to the `merge` function, which generally behaves the same as `{ ...existing, ...incoming }`, except when the `incoming` fields have custom `merge` functions. When `options.mergeObjects` encounters custom `merge` functions for any of the fields in its second argument (`incoming`), those nested `merge` functions will be called before combining the fields of `existing` and `incoming`, as desired:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        author: {\n          merge(existing, incoming, { mergeObjects }) {\n            // Correct, thanks to invoking nested merge functions.\n            return mergeObjects(existing, incoming);\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nBecause this `Book.author` field policy has no `Book`- or `Author`-specific logic in it, you can reuse this `merge` function for any field that needs this kind of handling.\n\nSince writing this kind of `merge` function can become repetitive, the following shorthand will provide the same behavior:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        author: {\n          // Short for options.mergeObjects(existing, incoming).\n          merge: true,\n        },\n      },\n    },\n  },\n});\n```\n\nIn summary, the `Book.author` policy above allows the cache to safely merge the `author` objects of any two `Book` objects that have the same identity.\n\n#### Configuring `merge` functions for types rather than fields\n\nBeginning with Apollo Client 3.3, you can avoid having to configure `merge` functions for lots of different fields that might hold an `Author` object, and instead put the `merge` configuration in the `Author` type policy:\n\n```ts{13}\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        // No longer necessary!\n        // author: {\n        //   merge: true,\n        // },\n      },\n    },\n\n    Author: {\n      merge: true,\n    },\n  },\n});\n```\n\nThese configurations have the same behavior, but putting the `merge: true` in the `Author` type policy is shorter and easier to maintain, especially when `Author` objects could appear in lots of different fields besides `Book.author`.\n\nRemember that mergeable objects will only be merged with existing objects occupying the same field of the same parent object, and only when the `__typename` of the objects is the same. If you really need to work around these rules, you can write a custom `merge` function to do whatever you want, but `merge: true` follows these rules.\n\n### Merging arrays of non-normalized objects\n\nOnce you're comfortable with the ideas and recommendations from the previous section, consider what happens when a `Book` can have multiple authors:\n\n```graphql\nquery BookWithAuthorNames {\n  favoriteBook {\n    isbn\n    title\n    authors {\n      name\n    }\n  }\n}\n\nquery BookWithAuthorLanguages {\n  favoriteBook {\n    isbn\n    title\n    authors {\n      language\n    }\n  }\n}\n```\n\nIn this case, the `favoriteBook.authors` field is no longer just a single object, but an array of authors, so it's even more imporant to define a custom `merge` function to prevent loss of data by replacement:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        authors: {\n          merge(existing: any[], incoming: any[], { readField, mergeObjects }) {\n            const merged: any[] = existing ? existing.slice(0) : [];\n            const authorNameToIndex: Record<string, number> = Object.create(null);\n            if (existing) {\n              existing.forEach((author, index) => {\n                authorNameToIndex[readField<string>(\"name\", author)] = index;\n              });\n            }\n            incoming.forEach(author => {\n              const name = readField<string>(\"name\", author);\n              const index = authorNameToIndex[name];\n              if (typeof index === \"number\") {\n                // Merge the new author data with the existing author data.\n                merged[index] = mergeObjects(merged[index], author);\n              } else {\n                // First time we've seen this author in this array.\n                authorNameToIndex[name] = merged.length;\n                merged.push(author);\n              }\n            });\n            return merged;\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nInstead of blindly replacing the existing `authors` array with the incoming array, this code concatenates the arrays together, while also checking for duplicate author names, merging the fields of any repeated `author` objects.\n\nThe `readField` helper function is more robust than using `author.name`, because it also tolerates the possibility that the `author` is a `Reference` object referring to data elsewhere in the cache, which could happen if you (or someone else on your team) eventually gets around to specifying `keyFields` for the `Author` type.\n\nAs this example suggests, `merge` functions can become quite sophisticated. When this happens, you can often extract the generic logic into a reusable helper function:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Book: {\n      fields: {\n        authors: {\n          merge: mergeArrayByField<AuthorType>(\"name\"),\n        },\n      },\n    },\n  },\n});\n```\n\nNow that you've hidden the details behind a reusable abstraction, it no longer matters how complicated the implementation gets. This is liberating, because it allows you to improve your client-side business logic over time, while keeping related logic consistent across your entire application.\n\n### Handling pagination\n\nWhen a field holds an array, it's often useful to [paginate](../pagination/overview/) that array's results, because the total result set can be arbitrarily large.\n\nTypically, a query includes pagination arguments that specify:\n\n* Where to start in the array, using either a numeric offset or a starting ID\n* The maximum number of elements to return in a single \"page\"\n\nIf you implement pagination for a field, it's important to keep pagination arguments in mind if you then implement `read` and `merge` functions for the field:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Agenda: {\n      fields: {\n        tasks: {\n          merge(existing: any[], incoming: any[], { args }) {\n            const merged = existing ? existing.slice(0) : [];\n            // Insert the incoming elements in the right places, according to args.\n            const end = args.offset + Math.min(args.limit, incoming.length);\n            for (let i = args.offset; i < end; ++i) {\n              merged[i] = incoming[i - args.offset];\n            }\n            return merged;\n          },\n\n          read(existing: any[], { args }) {\n            // If we read the field before any data has been written to the\n            // cache, this function will return undefined, which correctly\n            // indicates that the field is missing.\n            const page = existing && existing.slice(\n              args.offset,\n              args.offset + args.limit,\n            );\n            // If we ask for a page outside the bounds of the existing array,\n            // page.length will be 0, and we should return undefined instead of\n            // the empty array.\n            if (page && page.length > 0) {\n              return page;\n            }\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nAs this example shows, your `read` function often needs to cooperate with your `merge` function, by handling the same arguments in the inverse direction.\n\nIf you want a given \"page\" to start after a specific entity ID instead of starting from `args.offset`, you can implement your `merge` and `read` functions as follows, using the `readField` helper function to examine existing task IDs:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Agenda: {\n      fields: {\n        tasks: {\n          merge(existing: any[], incoming: any[], { args, readField }) {\n            const merged = existing ? existing.slice(0) : [];\n            // Obtain a Set of all existing task IDs.\n            const existingIdSet = new Set(\n              merged.map(task => readField(\"id\", task)));\n            // Remove incoming tasks already present in the existing data.\n            incoming = incoming.filter(\n              task => !existingIdSet.has(readField(\"id\", task)));\n            // Find the index of the task just before the incoming page of tasks.\n            const afterIndex = merged.findIndex(\n              task => args.afterId === readField(\"id\", task));\n            if (afterIndex >= 0) {\n              // If we found afterIndex, insert incoming after that index.\n              merged.splice(afterIndex + 1, 0, ...incoming);\n            } else {\n              // Otherwise insert incoming at the end of the existing data.\n              merged.push(...incoming);\n            }\n            return merged;\n          },\n\n          read(existing: any[], { args, readField }) {\n            if (existing) {\n              const afterIndex = existing.findIndex(\n                task => args.afterId === readField(\"id\", task));\n              if (afterIndex >= 0) {\n                const page = existing.slice(\n                  afterIndex + 1,\n                  afterIndex + 1 + args.limit,\n                );\n                if (page && page.length > 0) {\n                  return page;\n                }\n              }\n            }\n          },\n        },\n      },\n    },\n  },\n});\n```\n\nNote that if you call `readField(fieldName)`, it returns the value of the specified field from the current object. If you pass an object as a _second_ argument to `readField`, (e.g., `readField(\"id\", task)`), `readField` instead reads the specified field from the specified object. In the above example, reading the `id` field from existing `Task` objects allows us to deduplicate the `incoming` task data.\n\nThe pagination code above is complicated, but after you implement your preferred pagination strategy, you can reuse it for every field that uses that strategy, regardless of the field's type. For example:\n\n```ts\nfunction afterIdLimitPaginatedFieldPolicy<T>() {\n  return {\n    merge(existing: T[], incoming: T[], { args, readField }): T[] {\n      ...\n    },\n    read(existing: T[], { args, readField }): T[] {\n      ...\n    },\n  };\n}\n\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Agenda: {\n      fields: {\n        tasks: afterIdLimitPaginatedFieldPolicy<Reference>(),\n      },\n    },\n  },\n});\n```\n\n## Specifying key arguments\n\nIf a field accepts arguments, you can specify an array of `keyArgs` in the field's `FieldPolicy`. This array indicates which arguments are **key arguments** that are used to calculate the field's value. Specifying this array can help reduce the amount of duplicate data in your cache.\n\n### Example\n\nLet's say your schema's `Query` type includes a `monthForNumber` field. This field returns the details of particular month, given a provided `number` argument (January for `1` and so on). The `number` argument is a key argument for this field, because it is used when calculating the field's result:\n\n```ts\nconst cache = new InMemoryCache({\n  typePolicies: {\n    Query: {\n      fields: {\n        monthForNumber: {\n          keyArgs: [\"number\"],\n        },\n      },\n    },\n  },\n});\n```\n\nAn example of a _non-key_ argument is an access token, which is used to authorize a query but _not_ to calculate its result. If `monthForNumber` also accepts an `accessToken` argument, the value of that argument does _not_ affect which month's details are returned.\n\nBy default, the cache stores a separate value for _every unique combination of argument values you provide when querying a particular field_. When you specify a field's key arguments, the cache understands that any _non_-key arguments don't affect that field's value. Consequently, if you execute two different queries with the `monthForNumber` field, passing the _same_ `number` argument but _different_ `accessToken` arguments, the second query response will overwrite the first, because both invocations use the exact same value for all key arguments.\n\nIf you need more control over the behavior of `keyArgs`, you can pass a function instead of an array. This `keyArgs` function will receive the arguments object as its first parameter, and a `context` object providing other relevant details as its second parameter. See `KeyArgsFunction` in the types below for further information.\n\n## `FieldPolicy` API reference\n\nHere are the definitions for the `FieldPolicy` type and its related types:\n\n```ts\n// These generic type parameters will be inferred from the provided policy in\n// most cases, though you can use this type to constrain them more precisely.\ntype FieldPolicy<\n  TExisting,\n  TIncoming = TExisting,\n  TReadResult = TExisting,\n> = {\n  keyArgs?: KeySpecifier | KeyArgsFunction | false;\n  read?: FieldReadFunction<TExisting, TReadResult>;\n  merge?: FieldMergeFunction<TExisting, TIncoming> | boolean;\n};\n\ntype KeySpecifier = (string | KeySpecifier)[];\n\ntype KeyArgsFunction = (\n  args: Record<string, any> | null,\n  context: {\n    typename: string;\n    fieldName: string;\n    field: FieldNode | null;\n    variables?: Record<string, any>;\n  },\n) => string | KeySpecifier | null | void;\n\ntype FieldReadFunction<TExisting, TReadResult = TExisting> = (\n  existing: Readonly<TExisting> | undefined,\n  options: FieldFunctionOptions,\n) => TReadResult;\n\ntype FieldMergeFunction<TExisting, TIncoming = TExisting> = (\n  existing: Readonly<TExisting> | undefined,\n  incoming: Readonly<TIncoming>,\n  options: FieldFunctionOptions,\n) => TExisting;\n\n// These options are common to both read and merge functions:\ninterface FieldFunctionOptions {\n  cache: InMemoryCache;\n\n  // The final argument values passed to the field, after applying variables.\n  // If no arguments were provided, this property will be null.\n  args: Record<string, any> | null;\n\n  // The name of the field, equal to options.field.name.value when\n  // options.field is available. Useful if you reuse the same function for\n  // multiple fields, and you need to know which field you're currently\n  // processing. Always a string, even when options.field is null.\n  fieldName: string;\n\n  // The FieldNode object used to read this field. Useful if you need to\n  // know about other attributes of the field, such as its directives. This\n  // option will be null when a string was passed to options.readField.\n  field: FieldNode | null;\n\n  // The variables that were provided when reading the query that contained\n  // this field. Possibly undefined, if no variables were provided.\n  variables?: Record<string, any>;\n\n  // Easily detect { __ref: string } reference objects.\n  isReference(obj: any): obj is Reference;\n\n  // Returns a Reference object if obj can be identified, which requires,\n  // at minimum, a __typename and any necessary key fields. If true is\n  // passed for the optional mergeIntoStore argument, the object's fields\n  // will also be persisted into the cache, which can be useful to ensure\n  // the Reference actually refers to data stored in the cache. If you\n  // pass an ID string, toReference will make a Reference out of it. If\n  // you pass a Reference, toReference will return it as-is.\n  toReference(\n    objOrIdOrRef: StoreObject | string | Reference,\n    mergeIntoStore?: boolean,\n  ): Reference | undefined;\n\n  // Helper function for reading other fields within the current object.\n  // If a foreign object or reference is provided, the field will be read\n  // from that object instead of the current object, so this function can\n  // be used (together with isReference) to examine the cache outside the\n  // current object. If a FieldNode is passed instead of a string, and\n  // that FieldNode has arguments, the same options.variables will be used\n  // to compute the argument values. Note that this function will invoke\n  // custom read functions for other fields, if defined. Always returns\n  // immutable data (enforced with Object.freeze in development).\n  readField<T = StoreValue>(\n    nameOrField: string | FieldNode,\n    foreignObjOrRef?: StoreObject | Reference,\n  ): T;\n\n  // Returns true for non-normalized StoreObjects and non-dangling\n  // References, indicating that readField(name, objOrRef) has a chance of\n  // working. Useful for filtering out dangling references from lists.\n  canRead(value: StoreValue): boolean;\n\n  // A handy place to put field-specific data that you want to survive\n  // across multiple read function calls. Useful for field-level caching,\n  // if your read function does any expensive work.\n  storage: Record<string, any>;\n\n  // Instead of just merging objects with { ...existing, ...incoming }, this\n  // helper function can be used to merge objects in a way that respects any\n  // custom merge functions defined for their fields.\n  mergeObjects<T extends StoreObject | Reference>(\n    existing: T,\n    incoming: T,\n  ): T | undefined;\n}\n```\n"}},"__N_SSG":true}